{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeonju73/Image-Matching-System/blob/main/fine_tuning_cnn_ver3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchsummary"
      ],
      "metadata": {
        "id": "EjnHirnG_Ch2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "metadata": {
        "id": "Gg3lZN2BR-CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepair Dataset"
      ],
      "metadata": {
        "id": "2t_aNjhVtdk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hbcbh1999/recaptcha-dataset.git"
      ],
      "metadata": {
        "id": "IZZi5pnKTQQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./recaptcha-dataset/Large/Mountain/\n",
        "!rm -rf ./recaptcha-dataset/Large/Other/\n",
        "!rm -rf ./recaptcha-dataset/Large/readme.txt"
      ],
      "metadata": {
        "id": "PZHFOiRVT5ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uPdnGIJnM3AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ImageFolder structure\n",
        "\n",
        "```\n",
        "data_dir/Bicycle/xxx.png\n",
        "data_dir/Bicycle/xxy.png\n",
        "data_dir/Bicycle/[...]/xxz.png\n",
        "...\n",
        "data_dir/Traffic Light/123.png\n",
        "data_dir/Traffic Light/nsdf3.png\n",
        "data_dir/Traffic Light/[...]/asd932_.png\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SQcQwR6htyXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"./recaptcha-dataset/Large\"\n",
        "class_names = ['Bicycle', 'Bridge', 'Bus', 'Car',\n",
        "               'Chimney', 'Crosswalk', 'Hydrant',\n",
        "               'Motorcycle', 'Palm', 'Traffic Light']\n",
        "\n",
        "input_size = 224\n",
        "batch_size = 64\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(20),       # 회전\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 색 보정\n",
        "        transforms.RandomAffine(degrees=20, shear=10), # 전단\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "image_datasets = datasets.ImageFolder(data_dir, data_transforms)  # your dataset\n",
        "num_data = len(image_datasets)\n",
        "indices = np.arange(num_data)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_size = int(num_data*0.8)\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "train_set = torch.utils.data.Subset(image_datasets, train_indices)\n",
        "val_set = torch.utils.data.Subset(image_datasets, val_indices)\n",
        "\n",
        "print('Number of training data:', len(train_set))\n",
        "print('Number of validation data:', len(val_set))\n",
        "\n",
        "dataloaders = {'train': torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4),\n",
        "                 'val': torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4)}"
      ],
      "metadata": {
        "id": "f-a5q-mm4NDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(imgs, title=None):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    imgs = imgs.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    imgs = std * imgs + mean\n",
        "    imgs = np.clip(imgs, 0, 1)\n",
        "    plt.imshow(imgs)\n",
        "    if title is not None:\n",
        "        plt.title(title)"
      ],
      "metadata": {
        "id": "NjtPEI637SS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of training data\n",
        "inputs, labels = next(iter(dataloaders['train']))\n",
        "print(\"inputs.shape:\", inputs.shape)\n",
        "print(\"labels.shape:\", labels.shape)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[:8])\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in labels[:8]])"
      ],
      "metadata": {
        "id": "HVjOugffCZx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of validation data\n",
        "inputs, labels = next(iter(dataloaders['val']))\n",
        "print(\"inputs.shape:\", inputs.shape)\n",
        "print(\"labels.shape:\", labels.shape)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[:8])\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in labels[:8]])"
      ],
      "metadata": {
        "id": "159TxegOCeln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "H5HM_CPWCzF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet from scratch"
      ],
      "metadata": {
        "id": "RmfQKsLBD3UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![resnet](https://pytorch.org/assets/images/resnet.png)"
      ],
      "metadata": {
        "id": "fm1-PqFBOhRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WIkxG-ZuD8v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_18(nn.Module):\n",
        "\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "\n",
        "        super(ResNet_18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #resnet layers\n",
        "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
        "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
        "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
        "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=0.3)              # ← Dropout 추가\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def __make_layer(self, in_channels, out_channels, stride):\n",
        "\n",
        "        identity_downsample = None\n",
        "        if stride != 1:\n",
        "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
        "\n",
        "        return nn.Sequential(\n",
        "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride),\n",
        "            Block(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def identity_downsample(self, in_channels, out_channels):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )"
      ],
      "metadata": {
        "id": "ZdGXST_OQD_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet_18(image_channels=3, num_classes=10)\n",
        "summary(model, (3, 224, 224), device='cpu')\n",
        "# summary(model, (3, 512, 512), device='cpu')"
      ],
      "metadata": {
        "id": "hZe9KMhRSSJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet from model zoo"
      ],
      "metadata": {
        "id": "uSGb0H0LK4sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "Gn8XdwsxVFIF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "\n",
        "        # 2) feature_extract 여부에 따라 필요 없는 레이어는 freeze\n",
        "        feature_extract = True\n",
        "        if feature_extract:\n",
        "            for param in model_ft.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # 3) 새로운 헤드 정의\n",
        "        num_classes = len(image_datasets.classes)  # 예: 10개\n",
        "        dropout_p  = 0.5\n",
        "\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "        # resnet50 의 경우 avgpool → flatten → fc 였는데,\n",
        "        # 여기선 명시적으로 AdaptiveAvgPool2d → Dropout → Linear\n",
        "        model_ft.fc = nn.Sequential(\n",
        "            # nn.AdaptiveAvgPool2d((1,1)),  # (N,512,7,7) → (N,512,1,1)\n",
        "            nn.Flatten(),                 # (N,512,1,1) → (N,512)\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(num_ftrs, num_classes)\n",
        "        )\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size"
      ],
      "metadata": {
        "id": "C7w5Xp2vVOEg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"resnet\"\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = False\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "summary(model_ft, (3, 224, 224), device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMlCSruOTLay",
        "outputId": "46daf43a-03d4-419e-f306-6f1c451acc1d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "          Flatten-68                  [-1, 512]               0\n",
            "          Dropout-69                  [-1, 512]               0\n",
            "           Linear-70                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 5,130\n",
            "Non-trainable params: 11,176,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 106.02\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "SqloE-dVILSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "metadata": {
        "id": "BSJUGYaPVDSa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "LpyM6S03VXKO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# adam 이나 adamw 같은거 사용해도 됨\n",
        "optimizer_ft = optim.AdamW(params_to_update, lr=1e-4, weight_decay=1e-3)\n",
        "\n",
        "total_train_steps = len(dataloaders['train']) * num_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22iCsSbnVbiN",
        "outputId": "b7773e99-c98c-4c1e-f8b8-296f1a719ac6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t fc.2.weight\n",
            "\t fc.2.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the loss fxn\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "from collections import Counter\n",
        "\n",
        "# 1) 원본 ImageFolder 로드 (validation 분리 전)\n",
        "image_datasets = datasets.ImageFolder(data_dir, data_transforms)\n",
        "num_classes = len(image_datasets.classes)\n",
        "\n",
        "# 2) train/val split 을 수행할 때 썼던 인덱스(train_indices)를 재사용\n",
        "all_labels = np.array(image_datasets.targets)              # shape (N,)\n",
        "train_labels = all_labels[train_indices]                   # shape (N_train,)\n",
        "\n",
        "# 3) 클래스별 샘플 개수 세기\n",
        "counts = Counter(train_labels)                             # e.g. {0:823, 1:612, ...}\n",
        "class_counts = np.array([counts[i] for i in range(num_classes)], dtype=np.float32)\n",
        "\n",
        "# 4) inverse‐frequency 로 weight 계산\n",
        "#    weight[i] = 총 샘플 수 / (num_classes * class_counts[i])\n",
        "#    (CrossEntropyLoss 에 들어가는 weight 는 단순 비례값도 OK)\n",
        "class_weights = (train_labels.shape[0] / (num_classes * class_counts))\n",
        "class_weights = torch.from_numpy(class_weights).to(device) # GPU 위에 올려두면 편리\n",
        "\n",
        "# 5) criterion 정의 시에 weight 파라미터로 넘기기\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJZ2gUmNejXK",
        "outputId": "b8d5c3f1-e609-4b66-bd96-f3bad757e106"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 2.5800 Acc: 0.1264\n",
            "val Loss: 2.2962 Acc: 0.1535\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 2.3877 Acc: 0.1679\n",
            "val Loss: 2.0475 Acc: 0.3468\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 2.1248 Acc: 0.2619\n",
            "val Loss: 1.8055 Acc: 0.3871\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 1.9379 Acc: 0.3239\n",
            "val Loss: 1.6531 Acc: 0.4123\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 1.8756 Acc: 0.3429\n",
            "val Loss: 1.5994 Acc: 0.4949\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 1.7840 Acc: 0.3677\n",
            "val Loss: 1.5162 Acc: 0.4803\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 1.7764 Acc: 0.3711\n",
            "val Loss: 1.4612 Acc: 0.5294\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 1.6793 Acc: 0.3866\n",
            "val Loss: 1.4319 Acc: 0.5138\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 1.6859 Acc: 0.3877\n",
            "val Loss: 1.4279 Acc: 0.4696\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 1.6757 Acc: 0.3784\n",
            "val Loss: 1.4114 Acc: 0.5304\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 1.6907 Acc: 0.3825\n",
            "val Loss: 1.4377 Acc: 0.4954\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 1.6500 Acc: 0.4009\n",
            "val Loss: 1.4181 Acc: 0.5070\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 1.6453 Acc: 0.3940\n",
            "val Loss: 1.4293 Acc: 0.5153\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 1.6496 Acc: 0.3890\n",
            "val Loss: 1.3650 Acc: 0.5299\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 1.6324 Acc: 0.3968\n",
            "val Loss: 1.3358 Acc: 0.5206\n",
            "\n",
            "Training complete in 10m 55s\n",
            "Best val Acc: 0.530355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model & features"
      ],
      "metadata": {
        "id": "JRaNySNTepkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model_ft.state_dict(), 'resnet18.pt')"
      ],
      "metadata": {
        "id": "oMVfMNmwhvwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ft, 'resnet18_ft.pt')"
      ],
      "metadata": {
        "id": "hkqSDuKtiKc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = torch.load('resnet18_ft.pt')\n",
        "modules = list(model_ft.children())[:-1]\n",
        "resnet18_feat = nn.Sequential(*modules)\n",
        "for p in resnet18_feat.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for inputs, labels in dataloaders['val']:\n",
        "    inputs = inputs.to(device)\n",
        "    h = resnet18_feat(inputs)\n",
        "    # print(h.shape)      # [32, 512, 1, 1]\n",
        "\n",
        "    '''\n",
        "    code:\n",
        "    save the (features, labels)\n",
        "    '''"
      ],
      "metadata": {
        "id": "ak8zAYtSiVID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PsD8OrhJPcIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}